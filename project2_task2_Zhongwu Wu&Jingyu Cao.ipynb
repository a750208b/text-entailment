{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vectors_file = \"/Users/chriswoo/Desktop/640/project 2/glove.6B/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load glove_vectors into memory, deserializing the space separated format into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", errors='ignore') as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_contradiction(sentence1,sentence2):\n",
    "    c = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2: \n",
    "            if (((s1 == \"not\") or (s1==\"no\") or (s1 == \"none\")) or ((s2 == \"not\") or (s2==\"no\") or (s2 == \"none\"))):\n",
    "                #print(\"i have conjunction\",s1)\n",
    "                c = c+1\n",
    "                #print(c)\n",
    "                return c\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    contradiction=find_contradiction(word_tokenize(sentence1),word_tokenize(sentence2))\n",
    "    #print(\"contradiction is\", contradiction)\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    "    #print(sentence1)\n",
    "    #print(sentence2)\n",
    "   \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    #print(synsets1)\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    #print(synsets2)\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    #print(synsets1)\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "    #print(synsets2)\n",
    "    score, count = 0.0, 0\n",
    "    s_list = []\n",
    "    count_none = 0\n",
    "    for synset in synsets1:\n",
    "         #print(synset)\n",
    "         best = [synset.wup_similarity(ss) for ss in synsets2]\n",
    "         #print(best)\n",
    "         b = pd.Series(best).max()\n",
    "         s_list.append(b)\n",
    "    #print(\"similarity score is\", s_list)\n",
    "    scorelist = []\n",
    "    for s in s_list:\n",
    "       #print(s)\n",
    "       if s <= 1.0:\n",
    "           count_none = count_none + 1\n",
    "           scorelist.append(count_none)\n",
    "           #print(\"number of non none's are:\", count_none)\n",
    "           #print(\"number of nons are:\", (len(s_list)-count_none))\n",
    "           \n",
    "    #print(\"Total number of matches with less than or equal to 1 similarity:\", max(scorelist))\n",
    "    #print(\"Total number of nones:\", (len(s_list)-max(scorelist)))\n",
    "    #print(sum_list(s_list))\n",
    "    if contradiction == 1:\n",
    "        score = sum_list(s_list)/max(scorelist) - 1\n",
    "        #print(\"score for contradction is\",score)\n",
    "    else:\n",
    "        score = sum_list(s_list)/max(scorelist)\n",
    "        #print(\"score for neutral/entailment\",score)\n",
    "    return score\n",
    "\n",
    "def sum_list(l):\n",
    "    sum = 0\n",
    "    for x in l:\n",
    "        if x<= 1.0:\n",
    "             sum += x\n",
    "    return sum\n",
    "\n",
    "\n",
    " \n",
    "#for sentence in sentences:\n",
    " #   print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n",
    "    #print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\n",
    "    #print \n",
    "\n",
    "\n",
    "\n",
    "#is_anagram(\"The kids are playing outdoors near a man with a smile\", \"The young boys are playing outdoors and the man is smiling nearby\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants setup\n",
    "max_hypothesis_length, max_evidence_length = 30, 30\n",
    "batch_size, vector_size, hidden_size = 128, 50, 200 #INCREASED HIDDEN_SIZE FROM 64 TO 128\n",
    "\n",
    "lstm_size = hidden_size\n",
    "\n",
    "weight_decay = 0.001 # CHANGED FROM 0.0001\n",
    "\n",
    "learning_rate = 1.5\n",
    "\n",
    "input_p, output_p = 1.0, 1.0\n",
    "\n",
    "training_iterations_count = 120000\n",
    "\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            #print(\"hello\")\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    return rows, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_into_scores():\n",
    "\n",
    "    with open(\"/Users/chriswoo/Desktop/640/project 2/train.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"relatedness_score\"])\n",
    "            #scores.append(score_setup(row,labels))\n",
    "            #print(labels)\n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), labels, scores #, np.array(scores)\n",
    " \n",
    "data_feature_list, correct_labels, correct_score = split_data_into_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswoo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"sigmoid\", input_dim=1, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/Users/chriswoo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1356d32b0>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "#x = np.arange(-2,3.0,0.01)\n",
    "#y = x**2 - 2*x + 1\n",
    "\n",
    "correct_score1 = np.array(correct_score,dtype=float)\n",
    "correct_labels1 = np.array(correct_labels,dtype=float)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='sigmoid', \n",
    "                input_dim=1, init='uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(correct_score1,correct_labels1,nb_epoch=300, batch_size = 5,verbose = 0)\n",
    "\n",
    "#print(correct_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_into_scores():\n",
    "\n",
    "    with open(\"/Users/chriswoo/Desktop/640/project 2/trial.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"relatedness_score\"])\n",
    "            #scores.append(score_setup(row,labels))\n",
    "            #print(labels)\n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), labels, scores #, np.array(scores)\n",
    " \n",
    "data_feature_list1, correct_labels1, correct_score1 = split_data_into_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_score1=np.array(correct_score1)\n",
    "predict_relatedness1 = []\n",
    "for i in range(0,500):\n",
    "    predict_relatedness1.append(model.predict(np.asarray(correct_score1[i]).reshape(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial_label=[]\n",
    "for i in correct_labels1:\n",
    "    trial_label.append(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial_prediction=[]\n",
    "for i in predict_relatedness1:\n",
    "    trial_prediction.append(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5710534252297933, 1.3019645023657048e-44)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(trial_label,trial_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_score= np.array(correct_score).reshape(-1, 1)\n",
    "correct_labels = np.array(correct_labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.Lasso(alpha = 0.1)\n",
    "reg.fit(correct_score,correct_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_score1= np.array(correct_score1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_relatedness2=reg.predict(correct_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27415953030123663, 4.5204843739750397e-10)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(predict_relatedness2,trial_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswoo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"sigmoid\", input_dim=1, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/Users/chriswoo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"def predict_relatedness(score, labels):\\n   \\n    svr_len = SVR(kernel= 'linear', C=1e3)\\n    svr_len.fit(score,labels)\\n    return svr_len.predict\""
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "#x = np.arange(-2,3.0,0.01)\n",
    "#y = x**2 - 2*x + 1\n",
    "\n",
    "correct_score1 = np.array(correct_score,dtype=float)\n",
    "correct_labels1 = np.array(correct_labels,dtype=float)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='sigmoid', \n",
    "                input_dim=1, init='uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(correct_score1,correct_labels1,nb_epoch=300, batch_size = 5,verbose = 0)\n",
    "\n",
    "#print(correct_score)\n",
    "\n",
    "'''def predict_relatedness(score, labels):\n",
    "   \n",
    "    svr_len = SVR(kernel= 'linear', C=1e3)\n",
    "    svr_len.fit(score,labels)\n",
    "    return svr_len.predict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_into_scores1():\n",
    "\n",
    "    with open(\"/Users/chriswoo/Desktop/640/project 2/test.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        pair_id = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            #print(scores)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            pair_id.append(row[\"pair_ID\"])\n",
    "            #labels.append(row[\"relatedness_score\"])\n",
    "            #scores.append(score_setup(row,labels))\n",
    "            #print(labels)\n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), scores, pair_id #, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_feature_listt, correct_scoret,paid_idt = split_data_into_scores1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_scoret=np.array(correct_scoret)\n",
    "predict_relatedness = []\n",
    "for i in range(0,4927):\n",
    "    predict_relatedness.append(model.predict(np.asarray(correct_scoret[i]).reshape(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_DT = pd.DataFrame(data={\"pair_ID\":paid_idt,\"Relatedness_score\":predict_relatedness})\n",
    "output_DT.to_csv(\"/Users/chriswoo/Desktop/640/project 2/relatedness.csv\",index=False,quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "judgement=pd.read_csv('/Users/chriswoo/Desktop/640/project 2/judgment.csv')\n",
    "relatedness=pd.read_csv('/Users/chriswoo/Desktop/640/project 2/relatedness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final=judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final['Relatedness_score']=relatedness['Relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final= final[['pair_ID','entailment_judgment','Relatedness_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.to_csv('/Users/chriswoo/Desktop/640/project 2/result.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
