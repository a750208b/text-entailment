{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import dd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from nltk.wsd import lesk\n",
    "\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_contradiction(sentence1,sentence2):\n",
    "    c = 0\n",
    "    for s1 in sentence1:\n",
    "        for s2 in sentence2: \n",
    "            if (((s1 == \"not\") or (s1==\"no\") or (s1 == \"none\")) or ((s2 == \"not\") or (s2==\"no\") or (s2 == \"none\"))):\n",
    "                #print(\"i have conjunction\",s1)\n",
    "                c = c+1\n",
    "                #print(c)\n",
    "                return c\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    contradiction=find_contradiction(word_tokenize(sentence1),word_tokenize(sentence2))\n",
    "    #print(\"contradiction is\", contradiction)\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    "    #print(sentence1)\n",
    "    #print(sentence2)\n",
    "   \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    #print(synsets1)\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    #print(synsets2)\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    #print(synsets1)\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "    #print(synsets2)\n",
    "    score, count = 0.0, 0\n",
    "    s_list = []\n",
    "    count_none = 0\n",
    "    for synset in synsets1:\n",
    "         #print(synset)\n",
    "         best = [synset.wup_similarity(ss) for ss in synsets2]\n",
    "         #print(best)\n",
    "         b = pd.Series(best).max()\n",
    "         s_list.append(b)\n",
    "    #print(\"similarity score is\", s_list)\n",
    "    scorelist = []\n",
    "    for s in s_list:\n",
    "       #print(s)\n",
    "       if s <= 1.0:\n",
    "           count_none = count_none + 1\n",
    "           scorelist.append(count_none)\n",
    "           #print(\"number of non none's are:\", count_none)\n",
    "           #print(\"number of nons are:\", (len(s_list)-count_none))\n",
    "           \n",
    "    #print(\"Total number of matches with less than or equal to 1 similarity:\", max(scorelist))\n",
    "    #print(\"Total number of nones:\", (len(s_list)-max(scorelist)))\n",
    "    #print(sum_list(s_list))\n",
    "    if contradiction == 1:\n",
    "        score = sum_list(s_list)/max(scorelist) - 1\n",
    "        #print(\"score for contradction is\",score)\n",
    "    else:\n",
    "        score = sum_list(s_list)/max(scorelist)\n",
    "        #print(\"score for neutral/entailment\",score)\n",
    "    return score\n",
    "\n",
    "def sum_list(l):\n",
    "    sum = 0\n",
    "    for x in l:\n",
    "        if x<= 1.0:\n",
    "             sum += x\n",
    "    return sum\n",
    "\n",
    "\n",
    " \n",
    "#for sentence in sentences:\n",
    " #   print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n",
    "    #print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\n",
    "    #print \n",
    "\n",
    "\n",
    "\n",
    "#is_anagram(\"The kids are playing outdoors near a man with a smile\", \"The young boys are playing outdoors and the man is smiling nearby\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors_file = \"/Users/zoec/Documents/cus 640/PROJECT2/glove.6B.50d.txt\"\n",
    "\n",
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", errors='ignore') as glove:\n",
    "    for line in glove:\n",
    "        #print(\"hello\")\n",
    "        #print(line)\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        #print(name,vector)\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n",
    "        #print(glove_wordmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants setup\n",
    "max_hypothesis_length, max_evidence_length = 30, 30\n",
    "vector_size = 50#INCREASED HIDDEN_SIZE FROM 64 TO 128\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            #print(\"hello\")\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    return rows, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def split_data_into_scores():\n",
    "\n",
    "    with open(\"/Users/zoec/Documents/cus 640/PROJECT2/SICK_train.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        #labels1 = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "                        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), labels, scores #, np.array(scores)\n",
    " \n",
    "data_feature_list, correct_labels, correct_score = split_data_into_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = correct_score\n",
    "y_train= correct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=1, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.07% \n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "X1 = np.array(X_train,dtype=float)\n",
    "\n",
    "results = cross_val_score(estimator, X1, dummy_y, cv=kfold)\n",
    "estimator.fit(X1,dummy_y)\n",
    "\n",
    "print(\"Accuracy: %.2f%% \" % (results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def split_data_into_scores():\n",
    "\n",
    "    with open(\"/Users/zoec/Documents/cus 640/PROJECT2/SICK_trial.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        #labels1 = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "                        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), labels, scores #, np.array(scores)\n",
    " \n",
    "data_feature_list1, correct_labels1, correct_score1 = split_data_into_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=correct_score1\n",
    "y_test=correct_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7220000103116035"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(X_test,encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_pred)\n",
    "encoded_Y1 = encoder.transform(y_pred)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_predict = np_utils.to_categorical(encoded_Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= np.array(X_train).reshape(-1, 1)\n",
    "encoded_y_train = np.array(encoded_y_train).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train ,encoded_y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406666666666667"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,encoded_y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= np.array(X_test).reshape(-1, 1)\n",
    "encoded_y_test = np.array(encoded_y_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_test= np.array(encoded_y_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training set in SVM is 0.7386666666666667 \n"
     ]
    }
   ],
   "source": [
    "X_train= np.array(X_train).reshape(-1, 1)\n",
    "encoded_y_train = np.array(encoded_y_train).reshape(-1, 1)\n",
    "from sklearn import svm\n",
    "\n",
    "svm_clf = svm.SVC (kernel= 'rbf',C = 1)\n",
    "svm_clf.fit(X_train ,encoded_y_train)\n",
    "result=svm_clf.score(X_train,encoded_y_train)\n",
    "print(\"The accuracy of training set in SVM is %s \" %(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "\n",
    "glove_vectors_file1 = \"/Users/zoec/Documents/cus 640/PROJECT2/glove.6B.50d.txt\"\n",
    "\n",
    "glove_wordmap1 = {}\n",
    "with open(glove_vectors_file1, \"r\", errors='ignore') as glove:\n",
    "    for line in glove:\n",
    "        #print(\"hello\")\n",
    "        #print(line)\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        #print(name,vector)\n",
    "        glove_wordmap1[name] = np.fromstring(vector, sep=\" \")\n",
    "        #print(glove_wordmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2sequence1(sentence):\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            #print(\"hello\")\n",
    "            if word in glove_wordmap1:\n",
    "                rows.append(glove_wordmap1[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    return rows, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_scores1():\n",
    "\n",
    "    with open(\"/Users/zoec/Documents/cus 640/PROJECT2/SICK_test.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        pair_id = []\n",
    "        for row in train:\n",
    "            #print(row[\"sentence1\"])\n",
    "            focus_sentence = (row[\"sentence_A\"].lower())\n",
    "            sentences = (row[\"sentence_B\"].lower())\n",
    "            sc=sentence_similarity(focus_sentence,sentences)\n",
    "            #print(sc)\n",
    "            scores.append(sc)\n",
    "            #print(scores)\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            pair_id.append(row[\"pair_ID\"])\n",
    "            #labels.append(row[\"relatedness_score\"])\n",
    "            #scores.append(score_setup(row,labels))\n",
    "            #print(labels)\n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                      for x in evi_sentences])\n",
    "                             \n",
    "    return (hyp_sentences, evi_sentences), scores, pair_id #, np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_feature_listt, correct_scoret,paid_idt = split_data_into_scores1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_scoret=np.array(correct_scoret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = estimator.predict(correct_scoret)\n",
    "#predict_classification = []\n",
    "prediction_encoder = encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_DT = pd.DataFrame(data={\"pair_ID\":paid_idt,\"entailment_judgment\":prediction_encoder})\n",
    "output_DT.to_csv(\"/Users/zoec/Documents/cus 640/PROJECT2/judgment.csv\",index=False,quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
